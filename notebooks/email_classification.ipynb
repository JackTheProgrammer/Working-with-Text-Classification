{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from warnings import filterwarnings\n",
    "filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CATEGORY</th>\n",
       "      <th>MESSAGE</th>\n",
       "      <th>FILE_NAME</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Dear Homeowner,\\n\\n \\n\\nInterest Rates are at ...</td>\n",
       "      <td>00249.5f45607c1bffe89f60ba1ec9f878039a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>ATTENTION: This is a MUST for ALL Computer Use...</td>\n",
       "      <td>00373.ebe8670ac56b04125c25100a36ab0510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>This is a multi-part message in MIME format.\\n...</td>\n",
       "      <td>00214.1367039e50dc6b7adb0f2aa8aba83216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>IMPORTANT INFORMATION:\\n\\n\\n\\nThe new domain n...</td>\n",
       "      <td>00210.050ffd105bd4e006771ee63cabc59978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>This is the bottom line.  If you can GIVE AWAY...</td>\n",
       "      <td>00033.9babb58d9298daa2963d4f514193d7d6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5791</th>\n",
       "      <td>0</td>\n",
       "      <td>I'm one of the 30,000 but it's not working ver...</td>\n",
       "      <td>00609.dd49926ce94a1ea328cce9b62825bc97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5792</th>\n",
       "      <td>0</td>\n",
       "      <td>Damien Morton quoted:\\n\\n&gt;W3C approves HTML 4 ...</td>\n",
       "      <td>00957.e0b56b117f3ec5f85e432a9d2a47801f</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5793</th>\n",
       "      <td>0</td>\n",
       "      <td>On Mon, 2002-07-22 at 06:50, che wrote:\\n\\n\\n\\...</td>\n",
       "      <td>01127.841233b48eceb74a825417d8d918abf8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5794</th>\n",
       "      <td>0</td>\n",
       "      <td>Once upon a time, Manfred wrote :\\n\\n\\n\\n&gt; I w...</td>\n",
       "      <td>01178.5c977dff972cd6eef64d4173b90307f0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5795</th>\n",
       "      <td>0</td>\n",
       "      <td>If you run Pick, and then use the \"New FTOC\" b...</td>\n",
       "      <td>00747.352d424267d36975a7b40b85ffd0885e</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5796 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      CATEGORY                                            MESSAGE  \\\n",
       "0            1  Dear Homeowner,\\n\\n \\n\\nInterest Rates are at ...   \n",
       "1            1  ATTENTION: This is a MUST for ALL Computer Use...   \n",
       "2            1  This is a multi-part message in MIME format.\\n...   \n",
       "3            1  IMPORTANT INFORMATION:\\n\\n\\n\\nThe new domain n...   \n",
       "4            1  This is the bottom line.  If you can GIVE AWAY...   \n",
       "...        ...                                                ...   \n",
       "5791         0  I'm one of the 30,000 but it's not working ver...   \n",
       "5792         0  Damien Morton quoted:\\n\\n>W3C approves HTML 4 ...   \n",
       "5793         0  On Mon, 2002-07-22 at 06:50, che wrote:\\n\\n\\n\\...   \n",
       "5794         0  Once upon a time, Manfred wrote :\\n\\n\\n\\n> I w...   \n",
       "5795         0  If you run Pick, and then use the \"New FTOC\" b...   \n",
       "\n",
       "                                   FILE_NAME  \n",
       "0     00249.5f45607c1bffe89f60ba1ec9f878039a  \n",
       "1     00373.ebe8670ac56b04125c25100a36ab0510  \n",
       "2     00214.1367039e50dc6b7adb0f2aa8aba83216  \n",
       "3     00210.050ffd105bd4e006771ee63cabc59978  \n",
       "4     00033.9babb58d9298daa2963d4f514193d7d6  \n",
       "...                                      ...  \n",
       "5791  00609.dd49926ce94a1ea328cce9b62825bc97  \n",
       "5792  00957.e0b56b117f3ec5f85e432a9d2a47801f  \n",
       "5793  01127.841233b48eceb74a825417d8d918abf8  \n",
       "5794  01178.5c977dff972cd6eef64d4173b90307f0  \n",
       "5795  00747.352d424267d36975a7b40b85ffd0885e  \n",
       "\n",
       "[5796 rows x 3 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"../data/Spam Email raw text for NLP.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5796 entries, 0 to 5795\n",
      "Data columns (total 3 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   CATEGORY   5796 non-null   int64 \n",
      " 1   MESSAGE    5796 non-null   object\n",
      " 2   FILE_NAME  5796 non-null   object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 136.0+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5796 entries, 0 to 5795\n",
      "Data columns (total 3 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   CATEGORY   5796 non-null   int64 \n",
      " 1   MESSAGE    5796 non-null   string\n",
      " 2   FILE_NAME  5796 non-null   string\n",
      "dtypes: int64(1), string(2)\n",
      "memory usage: 136.0 KB\n"
     ]
    }
   ],
   "source": [
    "type_mapping = {\n",
    "    'MESSAGE': 'string',\n",
    "    'FILE_NAME': 'string'\n",
    "}\n",
    "df = df.astype(type_mapping)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CATEGORY\n",
       "0    3900\n",
       "1    1896\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['CATEGORY'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/fawad/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /Users/fawad/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\"HEY <LADIES>, <drop> it down. Just want to see you touch the ground. Don't be shy girl, go <BONANZA>. Shake your body like a belly dancer\",\n",
       " ['HEY',\n",
       "  'LADIES',\n",
       "  'drop',\n",
       "  'it',\n",
       "  'down',\n",
       "  'Just',\n",
       "  'want',\n",
       "  'to',\n",
       "  'see',\n",
       "  'you',\n",
       "  'touch',\n",
       "  'the',\n",
       "  'ground',\n",
       "  'Don',\n",
       "  't',\n",
       "  'be',\n",
       "  'shy',\n",
       "  'girl',\n",
       "  'go',\n",
       "  'BONANZA',\n",
       "  'Shake',\n",
       "  'your',\n",
       "  'body',\n",
       "  'like',\n",
       "  'a',\n",
       "  'belly',\n",
       "  'dancer'])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = nltk.RegexpTokenizer(r\"\\w+\")\n",
    "\n",
    "sentences = \"HEY <LADIES>, <drop> it down. Just want to see you touch the ground. Don't be shy girl, go <BONANZA>. Shake your body like a belly dancer\"\n",
    "\n",
    "tokenized_sentences = tokenizer.tokenize(sentences)\n",
    "sentences, tokenized_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hey',\n",
       " 'ladies',\n",
       " 'drop',\n",
       " 'it',\n",
       " 'down',\n",
       " 'just',\n",
       " 'want',\n",
       " 'to',\n",
       " 'see',\n",
       " 'you',\n",
       " 'touch',\n",
       " 'the',\n",
       " 'ground',\n",
       " 'don',\n",
       " 't',\n",
       " 'be',\n",
       " 'shy',\n",
       " 'girl',\n",
       " 'go',\n",
       " 'bonanza',\n",
       " 'shake',\n",
       " 'your',\n",
       " 'body',\n",
       " 'like',\n",
       " 'a',\n",
       " 'belly',\n",
       " 'dancer']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences_lower_cased = [t.lower() for t in tokenized_sentences]\n",
    "sentences_lower_cased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hey',\n",
       " 'lady',\n",
       " 'drop',\n",
       " 'it',\n",
       " 'down',\n",
       " 'just',\n",
       " 'want',\n",
       " 'to',\n",
       " 'see',\n",
       " 'you',\n",
       " 'touch',\n",
       " 'the',\n",
       " 'ground',\n",
       " 'don',\n",
       " 't',\n",
       " 'be',\n",
       " 'shy',\n",
       " 'girl',\n",
       " 'go',\n",
       " 'bonanza',\n",
       " 'shake',\n",
       " 'your',\n",
       " 'body',\n",
       " 'like',\n",
       " 'a',\n",
       " 'belly',\n",
       " 'dancer']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import WordNetLemmatizer\n",
    "\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_sentences = [wordnet_lemmatizer.lemmatize(token) for token in sentences_lower_cased]\n",
    "lemmatized_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hey',\n",
       " 'lady',\n",
       " 'drop',\n",
       " 'want',\n",
       " 'see',\n",
       " 'touch',\n",
       " 'ground',\n",
       " 'shy',\n",
       " 'girl',\n",
       " 'go',\n",
       " 'bonanza',\n",
       " 'shake',\n",
       " 'body',\n",
       " 'like',\n",
       " 'belly',\n",
       " 'dancer']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stopwords_list = stopwords.words('english')\n",
    "useful_tokens = [stopword for stopword in lemmatized_sentences if stopword not in stopwords_list]\n",
    "useful_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import WordNetLemmatizer\n",
    "\n",
    "def message_tokenizer(message):\n",
    "    reg_tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    tokenized_message = reg_tokenizer.tokenize(message)\n",
    "    lower_cased = [t.lower() for t in tokenized_message]\n",
    "    stop_words = stopwords.words('english')\n",
    "\n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_tokens = [wordnet_lemmatizer.lemmatize(lower_case) for lower_case in lower_cased]\n",
    "\n",
    "    required_tokens = [token for token in lemmatized_tokens if token not in stop_words]\n",
    "    return required_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hey',\n",
       " 'lady',\n",
       " 'drop',\n",
       " 'want',\n",
       " 'see',\n",
       " 'touch',\n",
       " 'ground',\n",
       " 'shy',\n",
       " 'girl',\n",
       " 'go',\n",
       " 'bonanza',\n",
       " 'shake',\n",
       " 'body',\n",
       " 'like',\n",
       " 'belly',\n",
       " 'dancer']"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "message_tokenizer(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train/Test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(      CATEGORY                                            MESSAGE  \\\n",
       " 0            1  \n",
       " \n",
       " <HTML><FONT  BACK=\"#ffffff\" style=\"BACKGROUN...   \n",
       " 1            1  <html><body bgColor=\"#CCCCCC\" topmargin=1 onMo...   \n",
       " 2            0  Quoting Paul Linehan (plinehan@yahoo.com):\n",
       " \n",
       " \n",
       " \n",
       " ...   \n",
       " 3            0  <a href=http://www.aaronsw.com/weblog/>\n",
       " \n",
       " Aaron...   \n",
       " 4            0  Oh yeah, the link for more info:\n",
       " \n",
       " \n",
       " \n",
       " http://www...   \n",
       " ...        ...                                                ...   \n",
       " 4631         0  Gregory Alan Bolcer:\n",
       " \n",
       " >I'm not sure since I ha...   \n",
       " 4632         1  New Account For: zzzz@spamassassin.taint.org\n",
       " \n",
       " ...   \n",
       " 4633         0  >>>>> \"O\" == Owen Byrne <owen@permafrost.net> ...   \n",
       " 4634         0  This is an automated response to a message you...   \n",
       " 4635         0  http://www.ouchytheclown.com/welcome.html\n",
       " \n",
       " \n",
       " \n",
       " \n",
       " ...   \n",
       " \n",
       "                                    FILE_NAME  \n",
       " 0     00118.141d803810acd9d4fc23db103dddfcd9  \n",
       " 1     00463.0bc4e08af0529dd773d9f10f922547db  \n",
       " 2     00358.87ee38040ac1f42320c7b89628b1850a  \n",
       " 3     01274.0d083a2d3b30061efdc2cc73ee9e76e3  \n",
       " 4     00756.2b2ec73ad20a4e0bdf31632ac019233b  \n",
       " ...                                      ...  \n",
       " 4631  00830.3a2cadbd29e654a7cbbf64ba4bdc378d  \n",
       " 4632  00354.dca4b8984863a76ffd01a33888498288  \n",
       " 4633  00346.f1d941485f6a20b29329111c59760585  \n",
       " 4634  00033.2ceb520d2c6500ccf24357f2ebdce618  \n",
       " 4635  00170.14c40e625814c14dfe2eb997157c6437  \n",
       " \n",
       " [4636 rows x 3 columns],\n",
       "       CATEGORY                                            MESSAGE  \\\n",
       " 0            0  This is just an semi-educated guess - if I'm w...   \n",
       " 1            1  ------=_NextPart_000_00B0_58C75D0E.A4523D08\n",
       " \n",
       " C...   \n",
       " 2            0  I seem to be getting the known spam message nu...   \n",
       " 3            0  \n",
       " \n",
       " \n",
       " \n",
       " >>>>> On Mon, 30 Sep 2002, \"Ted\" == Ted Ca...   \n",
       " 4            1  This is a multi-part message in MIME format.\n",
       " \n",
       " ...   \n",
       " ...        ...                                                ...   \n",
       " 1155         1  <html>\n",
       " \n",
       " \n",
       " \n",
       " <body>\n",
       " \n",
       " \n",
       " \n",
       " <font size=\"2\" PTSIZE=\"10\"...   \n",
       " 1156         0  \n",
       " \n",
       " \n",
       " \n",
       " formail did the trick. Thanks to those who...   \n",
       " 1157         0  URL: http://www.askbjoernhansen.com/archives/2...   \n",
       " 1158         1  <html>\n",
       " \n",
       " <head>\n",
       " \n",
       "    <meta http-equiv=3D\"Content...   \n",
       " 1159         0  >>>>> \"E\" == Elias Sinderson <elias@cse.ucsc.e...   \n",
       " \n",
       "                                    FILE_NAME  \n",
       " 0     01503.5e13994a5676296ed31b14e83367031c  \n",
       " 1     00441.3b9c3055e08bda4c0f7eea43749e324c  \n",
       " 2     00623.8bf6da05b986d3b16c208102e1c266f2  \n",
       " 3     01143.77077715a838bb473dad6a466d2e2403  \n",
       " 4     00224.1b3430b101a8a8b22493c4948fcbe9cc  \n",
       " ...                                      ...  \n",
       " 1155  00552.877d8dbff829787aa8349b433a8421f0  \n",
       " 1156  00647.97e77e8264c32c8b05077edc15721ba2  \n",
       " 1157  02055.80f7eff41824e0337e453a988ceda994  \n",
       " 1158  00376.f4ed5f002f9b6b320a67f1da9cacbe72  \n",
       " 1159  00987.1d700056f6a043acd5d388ca81fa0b1f  \n",
       " \n",
       " [1160 rows x 3 columns])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.sample(frac=1, random_state=1)\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "split_index = int(len(df) * 0.8)\n",
    "train_df, test_df = df[:split_index], df[split_index:]\n",
    "\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "test_df = test_df.reset_index(drop=True)\n",
    "\n",
    "train_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CATEGORY\n",
       "0    3112\n",
       "1    1524\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['CATEGORY'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CATEGORY\n",
       "0    788\n",
       "1    372\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df['CATEGORY'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'html': 4175,\n",
       "  'font': 35005,\n",
       "  'back': 1055,\n",
       "  'ffffff': 2535,\n",
       "  'style': 3349,\n",
       "  'background': 789,\n",
       "  'color': 9642,\n",
       "  'size': 13107,\n",
       "  '3': 3581,\n",
       "  'ptsize': 450,\n",
       "  '12': 985,\n",
       "  'b': 12856,\n",
       "  'viagra': 66,\n",
       "  '000000': 1923,\n",
       "  '2': 7993,\n",
       "  '10': 2182,\n",
       "  'family': 1491,\n",
       "  'sansserif': 314,\n",
       "  'face': 9950,\n",
       "  'arial': 6187,\n",
       "  'lang': 419,\n",
       "  '0': 9445,\n",
       "  'br': 16013,\n",
       "  'breakthrough': 22,\n",
       "  'medication': 50,\n",
       "  'impotence': 13,\n",
       "  'delivered': 79,\n",
       "  'mailbox': 71,\n",
       "  'without': 658,\n",
       "  'leaving': 50,\n",
       "  'computer': 640,\n",
       "  'simply': 377,\n",
       "  'click': 2144,\n",
       "  'href': 3875,\n",
       "  'http': 14926,\n",
       "  'host': 158,\n",
       "  '1bulk': 12,\n",
       "  'email': 4015,\n",
       "  'software': 1129,\n",
       "  'com': 11675,\n",
       "  'ch4': 12,\n",
       "  'pharm': 12,\n",
       "  'blue': 181,\n",
       "  'le': 680,\n",
       "  '5': 2932,\n",
       "  'minute': 366,\n",
       "  'complete': 403,\n",
       "  'line': 1307,\n",
       "  'consultation': 68,\n",
       "  'many': 1004,\n",
       "  'case': 681,\n",
       "  '24': 575,\n",
       "  'nbsp': 9732,\n",
       "  'hour': 589,\n",
       "  'gt': 108,\n",
       "  'website': 488,\n",
       "  'treatment': 33,\n",
       "  'compromised': 12,\n",
       "  'sexual': 120,\n",
       "  'function': 202,\n",
       "  'convenient': 36,\n",
       "  'affordable': 55,\n",
       "  'confidential': 135,\n",
       "  'ship': 80,\n",
       "  'worldwide': 100,\n",
       "  'u': 4258,\n",
       "  'price': 718,\n",
       "  'order': 1332,\n",
       "  'visit': 400,\n",
       "  'spam': 1140,\n",
       "  'receiving': 340,\n",
       "  'list': 6980,\n",
       "  'address': 1745,\n",
       "  'bought': 57,\n",
       "  'opted': 55,\n",
       "  'receive': 1064,\n",
       "  'information': 2084,\n",
       "  'business': 1982,\n",
       "  'opportunity': 520,\n",
       "  'opt': 242,\n",
       "  'please': 1795,\n",
       "  'accept': 161,\n",
       "  'apology': 77,\n",
       "  'removed': 723,\n",
       "  'reply': 592,\n",
       "  'remove': 1020,\n",
       "  'subject': 1227,\n",
       "  'never': 611,\n",
       "  'another': 564,\n",
       "  'mailto': 1070,\n",
       "  'remove432': 6,\n",
       "  'businessinfo': 6,\n",
       "  'center': 5282,\n",
       "  'body': 2000,\n",
       "  'bgcolor': 2847,\n",
       "  'cccccc': 141,\n",
       "  'topmargin': 91,\n",
       "  '1': 8998,\n",
       "  'onmouseover': 43,\n",
       "  'window': 852,\n",
       "  'status': 161,\n",
       "  'return': 429,\n",
       "  'true': 364,\n",
       "  'oncontextmenu': 15,\n",
       "  'false': 242,\n",
       "  'ondragstart': 13,\n",
       "  'onselectstart': 13,\n",
       "  'div': 3087,\n",
       "  'align': 6949,\n",
       "  'hello': 110,\n",
       "  'jlnax': 52,\n",
       "  'yahoo': 712,\n",
       "  'p': 10710,\n",
       "  '4': 3143,\n",
       "  'hum': 13,\n",
       "  'gro': 9,\n",
       "  'wth': 8,\n",
       "  'hor': 2,\n",
       "  'mone': 2,\n",
       "  'ther': 6,\n",
       "  'apy': 3,\n",
       "  'lo': 42,\n",
       "  'se': 127,\n",
       "  'wei': 14,\n",
       "  'ght': 13,\n",
       "  'buil': 2,\n",
       "  'ding': 17,\n",
       "  'mu': 9,\n",
       "  'cle': 7,\n",
       "  'mass': 197,\n",
       "  'rever': 3,\n",
       "  'sing': 6,\n",
       "  'ravag': 2,\n",
       "  'e': 2585,\n",
       "  'ag': 31,\n",
       "  'ing': 50,\n",
       "  'remar': 2,\n",
       "  'kable': 2,\n",
       "  'discov': 2,\n",
       "  'erie': 2,\n",
       "  'hormo': 2,\n",
       "  'ne': 79,\n",
       "  'hg': 7,\n",
       "  'h': 582,\n",
       "  'changing': 90,\n",
       "  'way': 1336,\n",
       "  'think': 1011,\n",
       "  'table': 4820,\n",
       "  'width': 9527,\n",
       "  '481': 6,\n",
       "  'tr': 10748,\n",
       "  'td': 17828,\n",
       "  'height': 4768,\n",
       "  '247': 28,\n",
       "  'left': 1493,\n",
       "  'helvetica': 3338,\n",
       "  'sans': 2997,\n",
       "  'serif': 3021,\n",
       "  'l': 662,\n",
       "  'ose': 5,\n",
       "  'bui': 2,\n",
       "  'ld': 10,\n",
       "  'tone': 27,\n",
       "  'verse': 3,\n",
       "  'agi': 6,\n",
       "  'ng': 68,\n",
       "  'increas': 2,\n",
       "  'ed': 232,\n",
       "  'lib': 150,\n",
       "  'ido': 2,\n",
       "  'dura': 2,\n",
       "  'tion': 7,\n",
       "  'pen': 12,\n",
       "  'ile': 9,\n",
       "  'erect': 9,\n",
       "  'ion': 30,\n",
       "  '222': 152,\n",
       "  'healt': 2,\n",
       "  'hier': 2,\n",
       "  'bon': 5,\n",
       "  'impr': 5,\n",
       "  'oved': 4,\n",
       "  'memo': 5,\n",
       "  'ry': 15,\n",
       "  'skin': 54,\n",
       "  'new': 2660,\n",
       "  'hair': 68,\n",
       "  'gr': 24,\n",
       "  'owth': 2,\n",
       "  'wri': 3,\n",
       "  'nkle': 2,\n",
       "  'disap': 2,\n",
       "  'pearance': 2,\n",
       "  '211': 230,\n",
       "  '99': 641,\n",
       "  '37': 136,\n",
       "  '206': 66,\n",
       "  '81': 415,\n",
       "  'ultimatehgh_li': 2,\n",
       "  'web': 972,\n",
       "  'si': 71,\n",
       "  'te': 74,\n",
       "  'learn': 294,\n",
       "  'fa': 245,\n",
       "  'ct': 47,\n",
       "  'rece': 2,\n",
       "  'iving': 2,\n",
       "  'subscr': 12,\n",
       "  'iber': 10,\n",
       "  'ameri': 12,\n",
       "  'ca': 599,\n",
       "  'mailin': 13,\n",
       "  'g': 654,\n",
       "  'li': 1710,\n",
       "  'remo': 15,\n",
       "  'self': 189,\n",
       "  'related': 175,\n",
       "  'mailli': 12,\n",
       "  'sts': 10,\n",
       "  'php': 466,\n",
       "  'userid': 26,\n",
       "  'quoting': 47,\n",
       "  'paul': 155,\n",
       "  'linehan': 11,\n",
       "  'plinehan': 6,\n",
       "  'confused': 57,\n",
       "  'thought': 318,\n",
       "  'wa': 3404,\n",
       "  'gpl': 179,\n",
       "  'money': 1456,\n",
       "  'paid': 229,\n",
       "  'suse': 136,\n",
       "  '60': 294,\n",
       "  'day': 1405,\n",
       "  'support': 589,\n",
       "  'whatever': 165,\n",
       "  'yes': 501,\n",
       "  'linux': 3132,\n",
       "  'distribution': 156,\n",
       "  '_any_': 4,\n",
       "  'contains': 244,\n",
       "  'lot': 495,\n",
       "  'separate': 85,\n",
       "  'codebases': 3,\n",
       "  'gnu': 223,\n",
       "  'clear': 200,\n",
       "  'point': 537,\n",
       "  'second': 385,\n",
       "  'try': 514,\n",
       "  'one': 3276,\n",
       "  'suspect': 62,\n",
       "  'actually': 453,\n",
       "  'understand': 238,\n",
       "  'perfectly': 50,\n",
       "  'like': 2068,\n",
       "  'give': 704,\n",
       "  'right': 2329,\n",
       "  'get': 3134,\n",
       "  'covered': 38,\n",
       "  'free': 2877,\n",
       "  'doe': 1011,\n",
       "  'provides': 161,\n",
       "  '_if_': 6,\n",
       "  'lawfully': 14,\n",
       "  'received': 550,\n",
       "  'copy': 490,\n",
       "  'binary': 85,\n",
       "  'version': 802,\n",
       "  'also': 1498,\n",
       "  'matching': 50,\n",
       "  'source': 571,\n",
       "  'code': 705,\n",
       "  'particularly': 74,\n",
       "  'need': 1401,\n",
       "  'really': 651,\n",
       "  'interested': 251,\n",
       "  'purchasing': 38,\n",
       "  'set': 783,\n",
       "  'disk': 223,\n",
       "  'billion': 223,\n",
       "  'apps': 61,\n",
       "  'use': 2257,\n",
       "  'nonetheless': 14,\n",
       "  'want': 1489,\n",
       "  'boxed': 30,\n",
       "  'content': 2903,\n",
       "  'copyright': 262,\n",
       "  'violation': 32,\n",
       "  '_and_': 3,\n",
       "  'pay': 416,\n",
       "  'purchase': 298,\n",
       "  'someone': 457,\n",
       "  '_give_': 2,\n",
       "  'lend': 10,\n",
       "  'duplicate': 50,\n",
       "  'thanks': 416,\n",
       "  'input': 1550,\n",
       "  'anyway': 183,\n",
       "  'friend': 432,\n",
       "  '7': 1261,\n",
       "  'pro': 108,\n",
       "  'ask': 191,\n",
       "  '_what_': 2,\n",
       "  'asking': 92,\n",
       "  'cd': 822,\n",
       "  'ripping': 9,\n",
       "  'illegally': 9,\n",
       "  'product': 1030,\n",
       "  'licensing': 54,\n",
       "  'write': 319,\n",
       "  'even': 1168,\n",
       "  'grab': 32,\n",
       "  '_from': 4,\n",
       "  'suse_': 4,\n",
       "  'heed': 5,\n",
       "  'term': 519,\n",
       "  'individual': 282,\n",
       "  'piece': 157,\n",
       "  'much': 1023,\n",
       "  'work': 1779,\n",
       "  'different': 485,\n",
       "  'clue': 96,\n",
       "  '_not_': 16,\n",
       "  'packaged': 18,\n",
       "  'retail': 161,\n",
       "  'rh': 120,\n",
       "  'include': 430,\n",
       "  'non': 462,\n",
       "  'freely': 34,\n",
       "  'redistributable': 7,\n",
       "  'application': 575,\n",
       "  'star': 88,\n",
       "  'office': 307,\n",
       "  'productivity': 18,\n",
       "  'among': 108,\n",
       "  'others': 326,\n",
       "  'core': 102,\n",
       "  'basic': 117,\n",
       "  'hand': 280,\n",
       "  'may': 1267,\n",
       "  'redistributed': 5,\n",
       "  'whether': 243,\n",
       "  'open': 452,\n",
       "  'hi': 305,\n",
       "  'signature': 513,\n",
       "  'virus': 121,\n",
       "  'help': 973,\n",
       "  'spread': 51,\n",
       "  'hilp': 1,\n",
       "  'sign': 172,\n",
       "  'turepread': 1,\n",
       "  'traped': 1,\n",
       "  'joe': 89,\n",
       "  'slater': 1,\n",
       "  'irish': 621,\n",
       "  'user': 2556,\n",
       "  'group': 1279,\n",
       "  'ilug': 1121,\n",
       "  'ie': 2163,\n",
       "  'www': 7291,\n",
       "  'mailman': 1701,\n",
       "  'listinfo': 2234,\n",
       "  'un': 677,\n",
       "  'subscription': 669,\n",
       "  'maintainer': 561,\n",
       "  'listmaster': 551,\n",
       "  'aaronsw': 19,\n",
       "  'weblog': 59,\n",
       "  'aaron': 6,\n",
       "  'sez': 5,\n",
       "  '30am': 5,\n",
       "  'ago': 250,\n",
       "  'server': 1047,\n",
       "  'seemed': 45,\n",
       "  'stop': 348,\n",
       "  'working': 412,\n",
       "  'could': 1150,\n",
       "  'ping': 28,\n",
       "  'anything': 366,\n",
       "  'else': 389,\n",
       "  'drove': 13,\n",
       "  'see': 1181,\n",
       "  'say': 948,\n",
       "  'everything': 343,\n",
       "  'broke': 23,\n",
       "  'repeatedly': 17,\n",
       "  'small': 532,\n",
       "  'idea': 481,\n",
       "  'evan': 1,\n",
       "  'grow': 56,\n",
       "  'mind': 259,\n",
       "  'call': 843,\n",
       "  'sysadmin': 13,\n",
       "  'oh': 161,\n",
       "  'yeah': 99,\n",
       "  'link': 1105,\n",
       "  'info': 588,\n",
       "  'microsoft': 342,\n",
       "  'windowsxp': 1,\n",
       "  'tabletpc': 4,\n",
       "  'default': 305,\n",
       "  'asp': 635,\n",
       "  'jim': 84,\n",
       "  'xent': 680,\n",
       "  'fork': 945,\n",
       "  'low': 403,\n",
       "  'cost': 540,\n",
       "  'life': 906,\n",
       "  'insurance': 529,\n",
       "  'save': 461,\n",
       "  '70': 278,\n",
       "  'policy': 409,\n",
       "  'male': 168,\n",
       "  'age': 300,\n",
       "  '40': 477,\n",
       "  '250': 201,\n",
       "  '000': 1846,\n",
       "  'year': 1596,\n",
       "  'level': 357,\n",
       "  '11': 780,\n",
       "  'per': 635,\n",
       "  'month': 800,\n",
       "  'quote': 294,\n",
       "  '78': 111,\n",
       "  '96': 100,\n",
       "  '242': 45,\n",
       "  'taken': 173,\n",
       "  'time': 3233,\n",
       "  'paying': 101,\n",
       "  'offer': 1058,\n",
       "  'lowest': 139,\n",
       "  'rate': 729,\n",
       "  'available': 702,\n",
       "  'nationally': 14,\n",
       "  'recognized': 45,\n",
       "  'carrier': 63,\n",
       "  'act': 358,\n",
       "  'simple': 402,\n",
       "  'removal': 249,\n",
       "  'instruction': 285,\n",
       "  'house': 198,\n",
       "  'htm': 698,\n",
       "  'enter': 170,\n",
       "  'unsubscribe': 674,\n",
       "  'thu': 190,\n",
       "  '2002': 2909,\n",
       "  '08': 413,\n",
       "  '15': 1022,\n",
       "  '53': 101,\n",
       "  'erik': 20,\n",
       "  'williamson': 1,\n",
       "  'wrote': 1340,\n",
       "  'problem': 968,\n",
       "  'making': 402,\n",
       "  'rpm': 1763,\n",
       "  'given': 248,\n",
       "  'sweet': 112,\n",
       "  'know': 1244,\n",
       "  'find': 944,\n",
       "  'officially': 22,\n",
       "  'ftp': 272,\n",
       "  'people': 2137,\n",
       "  'redhat': 704,\n",
       "  'ckloiber': 4,\n",
       "  'page': 734,\n",
       "  'rar': 2,\n",
       "  '00': 2069,\n",
       "  'file': 1539,\n",
       "  'archiving': 13,\n",
       "  'unarchiving': 1,\n",
       "  'program': 1154,\n",
       "  'distributed': 67,\n",
       "  'tarball': 13,\n",
       "  'repackaged': 6,\n",
       "  'based': 578,\n",
       "  'originally': 37,\n",
       "  'earlier': 76,\n",
       "  'found': 535,\n",
       "  'net': 5253,\n",
       "  'practice': 112,\n",
       "  'make': 1851,\n",
       "  'subpackages': 1,\n",
       "  'proud': 31,\n",
       "  'getting': 371,\n",
       "  'sickened': 1,\n",
       "  'crap': 42,\n",
       "  'enjoy': 96,\n",
       "  'chris': 239,\n",
       "  'kloiber': 11,\n",
       "  '_______________________________________________': 1181,\n",
       "  'mailing': 2085,\n",
       "  'freshrpms': 919,\n",
       "  'joseph': 54,\n",
       "  'barrera': 45,\n",
       "  'iii': 112,\n",
       "  'haun': 6,\n",
       "  'lifegem': 5,\n",
       "  'certified': 30,\n",
       "  'high': 584,\n",
       "  'quality': 278,\n",
       "  'diamond': 38,\n",
       "  'created': 134,\n",
       "  'carbon': 25,\n",
       "  'loved': 24,\n",
       "  'memorial': 15,\n",
       "  'unique': 135,\n",
       "  'wonderful': 55,\n",
       "  'wait': 170,\n",
       "  'dead': 108,\n",
       "  'sure': 574,\n",
       "  'enough': 332,\n",
       "  'fat': 117,\n",
       "  'typical': 36,\n",
       "  'liposuction': 3,\n",
       "  'job': 408,\n",
       "  'decent': 33,\n",
       "  'hell': 68,\n",
       "  'excrement': 1,\n",
       "  'love': 231,\n",
       "  'able': 427,\n",
       "  'sun': 202,\n",
       "  'shine': 2,\n",
       "  'occasional': 14,\n",
       "  'owen': 61,\n",
       "  'neil': 20,\n",
       "  'schemenauer': 5,\n",
       "  'result': 554,\n",
       "  'timtest': 24,\n",
       "  'py': 88,\n",
       "  'got': 487,\n",
       "  'three': 357,\n",
       "  'ham': 170,\n",
       "  '500': 441,\n",
       "  'message': 2385,\n",
       "  'happens': 74,\n",
       "  'enable': 94,\n",
       "  'latest': 143,\n",
       "  'header': 252,\n",
       "  'still': 764,\n",
       "  'summary': 80,\n",
       "  'cv': 186,\n",
       "  'running': 368,\n",
       "  'cmp': 7,\n",
       "  'process': 309,\n",
       "  'generalizing': 5,\n",
       "  'managed': 66,\n",
       "  'skip': 102,\n",
       "  'half': 209,\n",
       "  'wink': 49,\n",
       "  'n': 756,\n",
       "  'pair': 29,\n",
       "  'error': 531,\n",
       "  '6': 1517,\n",
       "  'f': 724,\n",
       "  'positive': 114,\n",
       "  'percentage': 80,\n",
       "  '187': 28,\n",
       "  'tied': 94,\n",
       "  '749': 5,\n",
       "  '562': 7,\n",
       "  '97': 93,\n",
       "  '780': 15,\n",
       "  '585': 4,\n",
       "  '25': 712,\n",
       "  'lost': 190,\n",
       "  'total': 281,\n",
       "  'fp': 34,\n",
       "  'went': 196,\n",
       "  '19': 610,\n",
       "  '17': 408,\n",
       "  'negative': 112,\n",
       "  '072': 7,\n",
       "  '318': 12,\n",
       "  '36': 232,\n",
       "  '39': 229,\n",
       "  '448': 4,\n",
       "  '46': 130,\n",
       "  '16': 435,\n",
       "  '574': 10,\n",
       "  '765': 3,\n",
       "  '33': 215,\n",
       "  '28': 294,\n",
       "  'fn': 16,\n",
       "  '43': 215,\n",
       "  'look': 743,\n",
       "  'promising': 9,\n",
       "  'output': 87,\n",
       "  'block': 104,\n",
       "  'would': 2081,\n",
       "  'clearer': 15,\n",
       "  'picture': 186,\n",
       "  'course': 450,\n",
       "  'anthony': 45,\n",
       "  'counting': 46,\n",
       "  'seem': 219,\n",
       "  'test': 353,\n",
       "  'data': 622,\n",
       "  'sigh': 21,\n",
       "  'fri': 165,\n",
       "  '9': 883,\n",
       "  'aug': 442,\n",
       "  '59': 178,\n",
       "  '0700': 141,\n",
       "  'pdt': 18,\n",
       "  'joshua': 4,\n",
       "  'daniel': 51,\n",
       "  'franklin': 15,\n",
       "  'joshuadfranklin': 1,\n",
       "  'sort': 212,\n",
       "  'discussion': 81,\n",
       "  'better': 589,\n",
       "  'gnome': 181,\n",
       "  'hate': 60,\n",
       "  'complain': 21,\n",
       "  'thread': 121,\n",
       "  'ha': 2745,\n",
       "  'going': 542,\n",
       "  'several': 322,\n",
       "  'deleting': 39,\n",
       "  'every': 751,\n",
       "  'reason': 357,\n",
       "  'spose': 1,\n",
       "  'brian': 104,\n",
       "  'fahrlÃ¤nder': 28,\n",
       "  'zealot': 30,\n",
       "  'conservative': 61,\n",
       "  'technomad': 28,\n",
       "  'evansville': 28,\n",
       "  'voyage': 30,\n",
       "  'countermoon': 28,\n",
       "  'icq': 60,\n",
       "  '5119262': 28,\n",
       "  'waddling': 3,\n",
       "  'mainstream': 12,\n",
       "  'suppose': 52,\n",
       "  'usatoday': 6,\n",
       "  'usatonline': 3,\n",
       "  '20020805': 20,\n",
       "  '4333165s': 3,\n",
       "  'wondering': 72,\n",
       "  'anyone': 641,\n",
       "  'experiening': 2,\n",
       "  'difficulty': 29,\n",
       "  'eircom': 45,\n",
       "  'mail': 2584,\n",
       "  'sever': 3,\n",
       "  'trying': 315,\n",
       "  'send': 1125,\n",
       "  'mozilla': 62,\n",
       "  'keep': 523,\n",
       "  'coming': 140,\n",
       "  'sorry': 144,\n",
       "  'domain': 279,\n",
       "  'isnt': 9,\n",
       "  'allows': 136,\n",
       "  'rcpthosts': 2,\n",
       "  'check': 906,\n",
       "  'recipient': 104,\n",
       "  'using': 1022,\n",
       "  'mail2': 7,\n",
       "  'smtp': 98,\n",
       "  'pop': 77,\n",
       "  'recieve': 29,\n",
       "  'cannot': 242,\n",
       "  'relay': 35,\n",
       "  'eircons': 1,\n",
       "  'end': 721,\n",
       "  'occasionally': 19,\n",
       "  'sending': 271,\n",
       "  'whose': 102,\n",
       "  'admin': 247,\n",
       "  'said': 973,\n",
       "  'known': 222,\n",
       "  'bug': 274,\n",
       "  'apply': 248,\n",
       "  'patch': 139,\n",
       "  'probably': 388,\n",
       "  'tell': 391,\n",
       "  'eircon': 2,\n",
       "  'admins': 7,\n",
       "  'already': 383,\n",
       "  'done': 340,\n",
       "  'lame': 15,\n",
       "  'answer': 364,\n",
       "  'long': 638,\n",
       "  'forgotten': 28,\n",
       "  'detail': 297,\n",
       "  'fun': 117,\n",
       "  'though': 400,\n",
       "  'rather': 287,\n",
       "  'interesting': 177,\n",
       "  'first': 1227,\n",
       "  'bit': 387,\n",
       "  'coverage': 81,\n",
       "  'liked': 29,\n",
       "  'salon': 13,\n",
       "  'mwt': 3,\n",
       "  'feature': 361,\n",
       "  '09': 894,\n",
       "  'forbidden_letters': 3,\n",
       "  'index': 531,\n",
       "  '22': 3128,\n",
       "  '0100': 166,\n",
       "  'nick': 33,\n",
       "  'hilliard': 1,\n",
       "  'possible': 372,\n",
       "  'silly': 40,\n",
       "  'question': 572,\n",
       "  'adsl': 16,\n",
       "  'service': 1217,\n",
       "  'nat': 19,\n",
       "  'unfiltered': 1,\n",
       "  'access': 539,\n",
       "  'real': 623,\n",
       "  'dynamic': 58,\n",
       "  'ip': 146,\n",
       "  'implication': 30,\n",
       "  'voip': 7,\n",
       "  'connecting': 19,\n",
       "  'client': 494,\n",
       "  'connected': 41,\n",
       "  'network': 674,\n",
       "  'internet': 1167,\n",
       "  'wild': 38,\n",
       "  'machine': 268,\n",
       "  'static': 25,\n",
       "  'suffer': 15,\n",
       "  'latency': 8,\n",
       "  'dl': 54,\n",
       "  'typically': 34,\n",
       "  'talking': 143,\n",
       "  '50ms': 1,\n",
       "  'rtt': 3,\n",
       "  'local': 342,\n",
       "  'ba': 525,\n",
       "  'pretty': 201,\n",
       "  'handle': 120,\n",
       "  'ok': 249,\n",
       "  'deal': 282,\n",
       "  'place': 497,\n",
       "  'used': 754,\n",
       "  'dsl': 37,\n",
       "  'read': 518,\n",
       "  'story': 352,\n",
       "  'allow': 300,\n",
       "  'greater': 89,\n",
       "  'distance': 89,\n",
       "  'something': 626,\n",
       "  'however': 485,\n",
       "  'knowledge': 151,\n",
       "  'physic': 29,\n",
       "  'newtonian': 1,\n",
       "  'worsening': 1,\n",
       "  'possibly': 61,\n",
       "  'improve': 99,\n",
       "  'reliability': 27,\n",
       "  '2000': 204,\n",
       "  'foot': 97,\n",
       "  'copper': 3,\n",
       "  'perhaps': 174,\n",
       "  'stretching': 6,\n",
       "  'space': 224,\n",
       "  'continuum': 2,\n",
       "  'explain': 73,\n",
       "  'word': 511,\n",
       "  'five': 182,\n",
       "  'syllable': 2,\n",
       "  'iiu': 140,\n",
       "  'taint': 245,\n",
       "  'org': 1421,\n",
       "  'antoin': 4,\n",
       "  'lachtnain': 2,\n",
       "  'eire': 4,\n",
       "  '353': 72,\n",
       "  '87': 220,\n",
       "  '240': 41,\n",
       "  '6691': 2,\n",
       "  'expand': 67,\n",
       "  'easy': 581,\n",
       "  'customer': 507,\n",
       "  'visa': 75,\n",
       "  'master': 98,\n",
       "  'card': 639,\n",
       "  'discover': 143,\n",
       "  'american': 410,\n",
       "  'express': 77,\n",
       "  'debit': 9,\n",
       "  'via': 315,\n",
       "  'phone': 1021,\n",
       "  'fax': 444,\n",
       "  'merchant': 88,\n",
       "  'account': 483,\n",
       "  'providing': 96,\n",
       "  'multiple': 188,\n",
       "  'method': 427,\n",
       "  'acceptable': 22,\n",
       "  'payment': 220,\n",
       "  'automatically': 176,\n",
       "  'ensure': 112,\n",
       "  'easier': 135,\n",
       "  'build': 503,\n",
       "  'trust': 210,\n",
       "  'cash': 329,\n",
       "  'dangerous': 75,\n",
       "  'absolutely': 224,\n",
       "  'guarantee': 256,\n",
       "  'whatsoever': 38,\n",
       "  'setup': 164,\n",
       "  'fee': 187,\n",
       "  'monthly': 124,\n",
       "  'obtain': 44,\n",
       "  'name': 3388,\n",
       "  'number': 952,\n",
       "  'area': 340,\n",
       "  'good': 967,\n",
       "  'contacted': 42,\n",
       "  'within': 666,\n",
       "  'staff': 73,\n",
       "  'thank': 297,\n",
       "  'contact': 546,\n",
       "  'looking': 513,\n",
       "  'might': 507,\n",
       "  'home': 1273,\n",
       "  'position': 197,\n",
       "  'involves': 26,\n",
       "  'week': 673,\n",
       "  'expect': 118,\n",
       "  'worked': 143,\n",
       "  'description': 96,\n",
       "  'go': 1120,\n",
       "  'begintodaystarttomorrow': 1,\n",
       "  's5': 2,\n",
       "  'great': 513,\n",
       "  'deathtospamdeathtospamdeathtospam': 128,\n",
       "  'sf': 932,\n",
       "  'sponsored': 580,\n",
       "  'jabber': 173,\n",
       "  'world': 1210,\n",
       "  'fastest': 142,\n",
       "  'growing': 211,\n",
       "  'communication': 541,\n",
       "  'platform': 217,\n",
       "  'im': 146,\n",
       "  'osdn': 216,\n",
       "  'xim': 87,\n",
       "  'spamassassin': 1678,\n",
       "  'sighting': 439,\n",
       "  'sourceforge': 1291,\n",
       "  'past': 266,\n",
       "  'nothing': 347,\n",
       "  'reporting': 74,\n",
       "  'seems': 362,\n",
       "  'database': 349,\n",
       "  'report': 967,\n",
       "  'response': 408,\n",
       "  'none': 474,\n",
       "  'seeing': 79,\n",
       "  'thinkgeek': 622,\n",
       "  'welcome': 379,\n",
       "  'geek': 356,\n",
       "  'heaven': 468,\n",
       "  'razor': 1195,\n",
       "  'padraig': 19,\n",
       "  'brady': 23,\n",
       "  'vincent': 19,\n",
       "  'cunniffe': 9,\n",
       "  'amount': 264,\n",
       "  'memory': 159,\n",
       "  'stupid': 86,\n",
       "  'start': 639,\n",
       "  '50': 1651,\n",
       "  'usage': 84,\n",
       "  '411436': 2,\n",
       "  'vsz': 6,\n",
       "  '840': 12,\n",
       "  'r': 1239,\n",
       "  'stay': 93,\n",
       "  '821036': 2,\n",
       "  '1040': 3,\n",
       "  'neither': 58,\n",
       "  'freeing': 8,\n",
       "  'resource': 213,\n",
       "  'correctly': 60,\n",
       "  'reusing': 2,\n",
       "  'issue': 470,\n",
       "  'pthreads': 4,\n",
       "  'solution': 265,\n",
       "  'yet': 335,\n",
       "  'err': 24,\n",
       "  'fixed': 219,\n",
       "  'seperate': 11,\n",
       "  'pthread_t': 3,\n",
       "  'tested': 50,\n",
       "  'identical': 29,\n",
       "  'unfortunately': 76,\n",
       "  'regard': 252,\n",
       "  'vin': 12,\n",
       "  'protect': 200,\n",
       "  'extended': 103,\n",
       "  'warranty': 118,\n",
       "  'car': 202,\n",
       "  'minivan': 4,\n",
       "  'truck': 32,\n",
       "  'suv': 19,\n",
       "  'buy': 399,\n",
       "  'direct': 196,\n",
       "  'extend': 30,\n",
       "  'existing': 130,\n",
       "  'original': 451,\n",
       "  'expired': 11,\n",
       "  'obligation': 180,\n",
       "  'investment': 386,\n",
       "  'fill': 341,\n",
       "  'form': 1201,\n",
       "  'hey11': 24,\n",
       "  'heyyy': 24,\n",
       "  '72': 332,\n",
       "  'multi': 313,\n",
       "  'part': 739,\n",
       "  'mime': 359,\n",
       "  'format': 386,\n",
       "  '_nextpart_000_005d_01c1be1c': 3,\n",
       "  '740aa160': 3,\n",
       "  'type': 2820,\n",
       "  'text': 2807,\n",
       "  'plain': 452,\n",
       "  'charset': 903,\n",
       "  'iso': 386,\n",
       "  '8859': 475,\n",
       "  'transfer': 679,\n",
       "  'encoding': 530,\n",
       "  'quoted': 285,\n",
       "  'printable': 263,\n",
       "  'alliance': 71,\n",
       "  'release': 433,\n",
       "  'sale': 501,\n",
       "  'plug': 54,\n",
       "  'play': 247,\n",
       "  'smart': 99,\n",
       "  'ground': 74,\n",
       "  'breaking': 39,\n",
       "  'biz': 126,\n",
       "  'op': 71,\n",
       "  'gold': 103,\n",
       "  'storming': 2,\n",
       "  'market': 616,\n",
       "  'bizoppinabox': 2,\n",
       "  'tampa': 2,\n",
       "  'florida': 46,\n",
       "  'january': 60,\n",
       "  '20': 6269,\n",
       "  'today': 850,\n",
       "  'announced': 66,\n",
       "  'state': 1430,\n",
       "  'art': 118,\n",
       "  'marketing': 605,\n",
       "  'technology': 571,\n",
       "  'training': 194,\n",
       "  'entrepreneur': 80,\n",
       "  'brings': 35,\n",
       "  'kindergarten': 2,\n",
       "  'includes': 258,\n",
       "  'licensed': 56,\n",
       "  'boiab': 4,\n",
       "  'explorer': 46,\n",
       "  'browser': 136,\n",
       "  'earth': 119,\n",
       "  'sprint': 21,\n",
       "  'dial': 74,\n",
       "  'cable': 124,\n",
       "  'comprehensive': 83,\n",
       "  'offline': 16,\n",
       "  'online': 684,\n",
       "  'industry': 319,\n",
       "  'ability': 183,\n",
       "  'update': 305,\n",
       "  'pace': 19,\n",
       "  'opp': 5,\n",
       "  'take': 980,\n",
       "  'next': 570,\n",
       "  'gary': 131,\n",
       "  'shawkey': 2,\n",
       "  'ceo': 40,\n",
       "  'pricing': 55,\n",
       "  'availability': 32,\n",
       "  '20th': 28,\n",
       "  '2001': 224,\n",
       "  'shipped': 56,\n",
       "  'starting': 167,\n",
       "  'february': 43,\n",
       "  '8th': 20,\n",
       "  'bizoppalliance': 3,\n",
       "  'member1849': 6,\n",
       "  'telephone': 131,\n",
       "  '800': 217,\n",
       "  '727': 2,\n",
       "  '6815': 2,\n",
       "  '1849': 2,\n",
       "  'authorized': 27,\n",
       "  'distributor': 40,\n",
       "  ...},\n",
       " 86415)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_count = {}\n",
    "\n",
    "for message in train_df['MESSAGE']:\n",
    "    tokenized_message = message_tokenizer(message)\n",
    "\n",
    "    for token in tokenized_message:\n",
    "        if token in token_count:\n",
    "            token_count[token] += 1\n",
    "        else:\n",
    "            token_count[token] = 1\n",
    "\n",
    "token_count, len(token_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keep_token(processed_token, threshold=10000):\n",
    "    if processed_token not in token_count:\n",
    "        return False\n",
    "    else:\n",
    "        return token_count[processed_token] > threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keep_token('quick', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['com', 'td', 'b', 'http', 'p', '3d', 'font', 'tr', 'br', 'size']"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = set()\n",
    "\n",
    "for token in token_count:\n",
    "    if keep_token(token, 9981) == True:\n",
    "        features.add(token)\n",
    "\n",
    "features = list(features)\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'com': 0,\n",
       " 'td': 1,\n",
       " 'b': 2,\n",
       " 'http': 3,\n",
       " 'p': 4,\n",
       " '3d': 5,\n",
       " 'font': 6,\n",
       " 'tr': 7,\n",
       " 'br': 8,\n",
       " 'size': 9}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_to_index_mapping = {t:i for t, i in zip(features, range(len(features)))}\n",
    "token_to_index_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['3d', 'b', 'br', 'com', 'bad', 'font', 'font', 'com', 'randoms']"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "message_tokenizer('3d b <br> .com bad font font com randoms')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of word approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**\"Bag of Words\" (count vector)**\n",
    "\n",
    "**-> T_s = [http  tr  size  3d  font  br  com  td   p   b]**\n",
    "\n",
    "**-> I_s = [0      1    2    3    4    5    6   7   8   9]**\n",
    "\n",
    "**-> V_s = [0,   0,   0,   1,  2,   1,   2,   0,  0,  1]**\n",
    "\n",
    "*Res*: `[0.,  0.,  0.,   1., 2.,  1., 2.,  0., 0., 1.]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def message_to_count_vector(message):\n",
    "    count_vector = np.zeros(len(features))\n",
    "    useful_tokens = message_tokenizer(message)\n",
    "    for token in useful_tokens:\n",
    "        if token not in features:\n",
    "            continue\n",
    "        token_index = token_to_index_mapping[token]\n",
    "        count_vector[token_index] += 1\n",
    "    return count_vector.astype('int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, 1, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "message_to_count_vector(train_df['MESSAGE'].iloc[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 1, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "message_to_count_vector(train_df['MESSAGE'].iloc[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_to_X_Y(df: pd.DataFrame):\n",
    "    y = df['CATEGORY'].to_numpy().astype('int64')\n",
    "    X = []\n",
    "    for message in df['MESSAGE']:\n",
    "        count_vector = message_to_count_vector(message=message)\n",
    "        X.append(count_vector)\n",
    "    return np.array(X).astype(int), np.array(y).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4636, 10), (4636,), (1160, 10), (1160,))"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, y_train = df_to_X_Y(df=train_df)\n",
    "X_test, y_test = df_to_X_Y(df=test_df)\n",
    "\n",
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling the `X_train` and `X_test`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 0.84303159, -0.2408693 , -0.077147  , ..., -0.26232183,\n",
       "          1.37277016, -0.07924315],\n",
       "        [-0.06741672,  0.00967368,  0.92077014, ..., -0.14917297,\n",
       "          0.86168575,  0.39972848],\n",
       "        [-0.19748076, -0.2408693 , -0.27673043, ..., -0.26232183,\n",
       "         -0.16048307, -0.2708318 ],\n",
       "        ...,\n",
       "        [-0.06741672, -0.2408693 , -0.27673043, ..., -0.26232183,\n",
       "         -0.16048307, -0.2708318 ],\n",
       "        [-0.06741672, -0.2408693 , -0.27673043, ..., -0.26232183,\n",
       "         -0.16048307, -0.2708318 ],\n",
       "        [ 0.19271137, -0.2408693 , -0.27673043, ..., -0.26232183,\n",
       "         -0.16048307, -0.2708318 ]]),\n",
       " array([[-0.08846002, -0.24243477, -0.28549788, ..., -0.25474501,\n",
       "         -0.16872934, -0.14736084],\n",
       "        [-0.2626767 , -0.24243477, -0.28549788, ..., -0.25474501,\n",
       "         -0.16872934, -0.14736084],\n",
       "        [-0.08846002, -0.24243477, -0.28549788, ..., -0.25474501,\n",
       "         -0.16872934, -0.14736084],\n",
       "        ...,\n",
       "        [-0.17556836, -0.24243477, -0.28549788, ..., -0.25474501,\n",
       "         -0.16872934, -0.14736084],\n",
       "        [-0.17556836,  0.38390005,  3.95527101, ...,  0.5927814 ,\n",
       "          0.35043787,  0.49937998],\n",
       "        [ 0.08575666, -0.24243477, -0.28549788, ..., -0.25474501,\n",
       "         -0.16872934, -0.14736084]]))"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "s_scaler = StandardScaler()\n",
    "X_train_scaled, X_test_scaled = s_scaler.fit_transform(X_train), s_scaler.fit_transform(X_test)\n",
    "X_train_scaled, X_test_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training classification models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.99      0.90       788\n",
      "           1       0.98      0.52      0.68       372\n",
      "\n",
      "    accuracy                           0.84      1160\n",
      "   macro avg       0.90      0.76      0.79      1160\n",
      "weighted avg       0.87      0.84      0.83      1160\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "svc = SVC(kernel='linear', gamma='scale', class_weight='balanced')\n",
    "svc.fit(X=X_train_scaled, y=y_train)\n",
    "predictions_svc = svc.predict(X=X_test_scaled)\n",
    "print(classification_report(y_pred=predictions_svc, y_true=y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      1.00      0.90       788\n",
      "           1       0.98      0.51      0.67       372\n",
      "\n",
      "    accuracy                           0.84      1160\n",
      "   macro avg       0.90      0.75      0.79      1160\n",
      "weighted avg       0.87      0.84      0.82      1160\n",
      "\n"
     ]
    }
   ],
   "source": [
    "svc = SVC(kernel='rbf', gamma='scale', class_weight='balanced')\n",
    "svc.fit(X=X_train_scaled, y=y_train)\n",
    "predictions_svc = svc.predict(X=X_test_scaled)\n",
    "print(classification_report(y_pred=predictions_svc, y_true=y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.99      0.90       788\n",
      "           1       0.98      0.53      0.69       372\n",
      "\n",
      "    accuracy                           0.85      1160\n",
      "   macro avg       0.90      0.76      0.79      1160\n",
      "weighted avg       0.87      0.85      0.83      1160\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "l_regression = LogisticRegression(penalty='l2', random_state=1, solver='saga', class_weight='balanced')\n",
    "pred_logistic = l_regression.fit(X=X_train_scaled, y=y_train).predict(X_test_scaled)\n",
    "print(classification_report(y_pred=pred_logistic, y_true=y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.93      0.89       788\n",
      "           1       0.81      0.65      0.72       372\n",
      "\n",
      "    accuracy                           0.84      1160\n",
      "   macro avg       0.83      0.79      0.80      1160\n",
      "weighted avg       0.83      0.84      0.83      1160\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(\n",
    "    n_jobs=300,\n",
    "    criterion='log_loss',\n",
    "    class_weight='balanced',\n",
    "    random_state=1,\n",
    "    max_features=len(features),\n",
    "    warm_start=True,\n",
    ").fit(X_train, y_train)\n",
    "print(classification_report(y_true=y_test, y_pred=rf.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you see it in all of those, you can see the class imbalance. Therefore, there has to be augmented data for non-spam (1) category."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Balanced data model training\n",
    "\n",
    "We'll create synthetic data by generating new samples based on the existing data points in the minority class. The **SMOTE (Synthetic Minority Over-sampling Technique)** algorithm is popular for generating synthetic data by interpolating between existing minority class samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.99      0.80       788\n",
      "           1       0.99      0.52      0.68       788\n",
      "\n",
      "    accuracy                           0.76      1576\n",
      "   macro avg       0.83      0.76      0.74      1576\n",
      "weighted avg       0.83      0.76      0.74      1576\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "smote = SMOTE(random_state=1)\n",
    "\n",
    "X_train_balanced, y_train_balanced = smote.fit_resample(X_train_scaled, y_train)\n",
    "X_test_balanced, y_test_balanced = smote.fit_resample(X_test_scaled, y_test)\n",
    "\n",
    "l_regression = LogisticRegression(penalty='l2', random_state=1, solver='saga', class_weight='balanced')\n",
    "pred_logistic_smote = l_regression.fit(X=X_train_balanced, y=y_train_balanced).predict(X_test_balanced)\n",
    "\n",
    "print(classification_report(y_pred=pred_logistic_smote, y_true=y_test_balanced))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like use of SMOTE to balance the classes had a noticeable impact on model's performance. Hereâ€™s what the output tells us:\n",
    "\n",
    "### Observations:\n",
    "- **Precision and Recall Trade-off**: Your recall for class `0` (majority class) is very high (0.99), indicating that the model is very good at identifying `0` instances. However, the precision for class `0` has decreased to 0.67, which means that when the model predicts `0`, it's correct about 67% of the time.\n",
    "- **Class `1` (minority class) Performance**: The precision for class `1` has improved to 0.99, which is excellent, but the recall is lower at 0.52. This indicates that while the model is very good at identifying when it predicts class `1`, it misses half of the actual class `1` instances.\n",
    "- **Overall Accuracy**: Your model's accuracy has dropped to 0.76, which may be a result of the imbalance correction affecting how well it generalizes to both classes.\n",
    "\n",
    "### Key Takeaways:\n",
    "1. **SMOTE has helped with balancing** the dataset by creating synthetic instances, improving the model's ability to identify the minority class (`1`) when it's predicted.\n",
    "2. **Precision and recall** are not in perfect balance; improving one reduces the other, so consider the specific use case and which metric matters more for your objectives.\n",
    "3. **Overfitting Potential**: You may be at risk of overfitting due to the synthetic data generated by SMOTE. This is especially true if the number of synthetic samples is high.\n",
    "\n",
    "### Recommendations for Improvement:\n",
    "- **Try Different Sampling Techniques**: You could experiment with **SMOTE-NC** (SMOTE for mixed-type data) or **ADASYN** for more adaptive oversampling.\n",
    "- **Use Ensemble Models**: Consider using ensemble methods like **Random Forest** or **Gradient Boosting** (e.g., `XGBoost` or `LightGBM`) with **balanced class weights**.\n",
    "- **Tune Hyperparameters**: Adjust hyperparameters for `LogisticRegression` (e.g., `C` value for regularization) and try techniques like **grid search** or **random search** for optimal results.\n",
    "- **Evaluate with Different Metrics**: If you're dealing with an imbalanced problem, focus more on metrics like **precision-recall AUC**, **F1-score**, and **confusion matrix** rather than just accuracy.\n",
    "\n",
    "### Next Steps:\n",
    "1. **Plot Precision-Recall Curve**: Visualize how precision and recall change with different threshold values.\n",
    "2. **Cross-Validation**: Ensure you're validating your model using cross-validation to get a more robust estimate of performance.\n",
    "3. **Try Class Weights**: Combining SMOTE with `class_weight='balanced'` may help optimize results without losing generality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.61      0.65       788\n",
      "           1       0.65      0.73      0.69       788\n",
      "\n",
      "    accuracy                           0.67      1576\n",
      "   macro avg       0.67      0.67      0.67      1576\n",
      "weighted avg       0.67      0.67      0.67      1576\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(\n",
    "    n_jobs=300,\n",
    "    criterion='entropy',\n",
    "    class_weight='balanced',\n",
    "    random_state=1,\n",
    "    max_features=len(features),\n",
    "    warm_start=True,\n",
    ")\\\n",
    "    .fit(X_train_balanced, y_train_balanced)\n",
    "print(classification_report(y_true=y_test_balanced, y_pred=rf.predict(X_test_balanced)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-Validation F1 Score: 0.7725919260601385\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "scores = cross_val_score(rf, X_train_balanced, y_train_balanced, cv=5, scoring='f1')\n",
    "print(f'Cross-Validation F1 Score: {scores.mean()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.92      0.78       788\n",
      "           1       0.88      0.56      0.68       788\n",
      "\n",
      "    accuracy                           0.74      1576\n",
      "   macro avg       0.78      0.74      0.73      1576\n",
      "weighted avg       0.78      0.74      0.73      1576\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "xgb_classifier = xgb.XGBClassifier(\n",
    "    n_estimators=1000,\n",
    "    n_jobs=200,\n",
    "    grow_policy='lossguide',\n",
    "    learning_rate=0.001,\n",
    "    booster='gbtree',\n",
    "    random_state=1,\n",
    "    tree_method='exact',\n",
    "    eval_metric='logloss'\n",
    ").fit(X=X_train_balanced, y=y_train_balanced)\n",
    "\n",
    "print(classification_report(y_true=y_test_balanced, y_pred=xgb_classifier.predict(X_test_balanced)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-Validation F1 Score: 0.74703133779682\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "scores = cross_val_score(xgb_classifier, X_train_balanced, y_train_balanced, cv=5, scoring='f1')\n",
    "print(f'Cross-Validation F1 Score: {scores.mean()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensembling based model training\n",
    "\n",
    "As of now, our best models with balanced dataset of being trained and tested are that of `l_regression` and `xgb_classifier`. We'll combine them up for the best of the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VotingClassifier\n",
    "\n",
    "A simple way to combine models by averaging their predictions. This method combines the predictions from multiple models and selects the most frequent class (hard voting) or the average of probabilities (soft voting) as the final prediction. Here's how you can do it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.98      0.80       788\n",
      "           1       0.96      0.53      0.68       788\n",
      "\n",
      "    accuracy                           0.75      1576\n",
      "   macro avg       0.82      0.75      0.74      1576\n",
      "weighted avg       0.82      0.75      0.74      1576\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "ensemble_model = VotingClassifier(\n",
    "    estimators=[\n",
    "        ('log_reg', l_regression),\n",
    "        ('xgb', xgb_classifier)\n",
    "    ],\n",
    "    voting='soft'\n",
    ")\n",
    "\n",
    "ensemble_model.fit(X_train_balanced, y_train_balanced)\n",
    "\n",
    "print(classification_report(y_true=y_test_balanced, y_pred=ensemble_model.predict(X_test_balanced)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-Validation F1 Score: 0.7066587760971305\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "scores = cross_val_score(ensemble_model, X_train_balanced, y_train_balanced, cv=5, scoring='f1')\n",
    "print(f'Cross-Validation F1 Score: {scores.mean()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## StackingClassifier\n",
    "\n",
    "Use the predictions of one set of models as input features for another model to learn how to combine them optimally. A stacking classifier combines the predictions of base models and uses a meta-model to find the optimal combination of them. This is helpful when you want a model to learn how to best combine the predictions from LogisticRegression and XGBoost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.90      0.78       788\n",
      "           1       0.86      0.58      0.69       788\n",
      "\n",
      "    accuracy                           0.74      1576\n",
      "   macro avg       0.77      0.74      0.73      1576\n",
      "weighted avg       0.77      0.74      0.73      1576\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import StackingClassifier\n",
    "\n",
    "base_models = [('l_regression', l_regression), ('xgb_classifier', xgb_classifier)]\n",
    "\n",
    "stacked_model = StackingClassifier(\n",
    "    estimators=base_models,\n",
    "    final_estimator=rf\n",
    ")\n",
    "stacked_model.fit(X_train_balanced, y_train_balanced)\n",
    "print(classification_report(y_true=y_test_balanced, y_pred=stacked_model.predict(X_test_balanced)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-Validation F1 Score: 0.740311276852135\n"
     ]
    }
   ],
   "source": [
    "scores = cross_val_score(stacked_model, X_train_balanced, y_train_balanced, cv=5, scoring='f1')\n",
    "print(f'Cross-Validation F1 Score: {scores.mean()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End notes\n",
    "While doing this project, I did the following mistakes:\n",
    " * **Not identifying the patterns of the spam and non-spam emails**: I should've first identified the pattern of token(s)' occurrences with both spam and non-spam kind of emails.\n",
    " * **Simulating randomness**: Instead of randomness by going for `df.sample`, I should've gone for suitable data augmentation technique.\n",
    " * **A good dataset**: This dataset is way too small for email classification, for spam classification, there're also emoji, phishing techniques involved as well, and other vice versa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are a few additional points where improvements could be made or explored further:\n",
    "\n",
    "---\n",
    "\n",
    "### **1. Lack of Data Preprocessing Evaluation**\n",
    "   - **Possible Issue**: Preprocessing choices like tokenization, lemmatization, and threshold-based inclusion may not align optimally with the nature of the dataset.\n",
    "   - **Suggested Improvement**: Evaluate whether preprocessing steps:\n",
    "     - Retain meaningful information.\n",
    "     - Don't inadvertently remove critical spam indicators (e.g., numbers, special characters, or URLs).\n",
    "     - Include domain-specific stopwords (e.g., \"click\", \"free\", \"offer\" for spam classification).\n",
    "   - Experiment with different preprocessing pipelines and validate their impact on model performance.\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Overlooking Model Interpretability**\n",
    "   - **Possible Issue**: The project could benefit from analyzing which features or tokens contribute most to predictions. Without this, we're working in a \"black box\" mode.\n",
    "   - **Suggested Improvement**:\n",
    "     - Use **SHAP (SHapley Additive exPlanations)** or **LIME (Local Interpretable Model-agnostic Explanations)** to identify the importance of specific tokens.\n",
    "     - Analyze token contributions for spam vs. non-spam predictions to uncover new insights into patterns and biases.\n",
    "\n",
    "---\n",
    "\n",
    "### **3. Focusing Solely on SMOTE Oversampling**\n",
    "   - **Possible Issue**: While SMOTE balances the dataset, it creates synthetic samples that might not accurately represent the data distribution, especially for high-dimensional data like text.\n",
    "   - **Suggested Improvement**:\n",
    "     - Compare SMOTE results with **undersampling**, **class-weight adjustments**, or **other oversampling techniques** like ADASYN.\n",
    "     - Incorporate **data augmentation** methods to increase diversity without relying solely on resampling algorithms.\n",
    "\n",
    "---\n",
    "\n",
    "### **4. Model-Specific Overfitting Risk**\n",
    "   - **Possible Issue**: With ensemble models like XGBoost and RandomForest, there's a chance of overfitting to the balanced dataset if hyperparameters arenâ€™t carefully tuned.\n",
    "   - **Suggested Improvement**:\n",
    "     - Use a **validation set** alongside cross-validation to detect overfitting.\n",
    "     - Regularize the model using appropriate parameters (e.g., `gamma` for XGBoost, `min_samples_split` for RandomForest).\n",
    "\n",
    "---\n",
    "\n",
    "### **5. Overlooking Ensemble Diversity**\n",
    "   - **Possible Issue**: While the ensemble approach with logistic regression and XGBoost is promising, both models might share similar biases.\n",
    "   - **Suggested Improvement**:\n",
    "     - Increase diversity in your ensemble by including fundamentally different algorithms (e.g., NaÃ¯ve Bayes or SVM with custom kernels).\n",
    "     - Use a meta-model (e.g., stacking with a Logistic Regression or a LightGBM as a meta-learner) to combine predictions more effectively.\n",
    "\n",
    "---\n",
    "\n",
    "### **6. Overreliance on Accuracy and F1 Scores**\n",
    "   - **Possible Issue**: Metrics like accuracy and F1-score might not fully capture the model's ability to differentiate between spam and non-spam emails.\n",
    "   - **Suggested Improvement**:\n",
    "     - Incorporate other evaluation metrics like:\n",
    "       - **Precision-Recall AUC**: Especially important for imbalanced datasets.\n",
    "       - **False Positive Rate (FPR)**: To check if the spam filter mistakenly classifies valid emails as spam.\n",
    "\n",
    "---\n",
    "\n",
    "### **7. Dataset Size vs. Complexity Trade-off**\n",
    "   - **Possible Issue**: Applying highly complex models like XGBoost on a small dataset might not fully leverage their capabilities.\n",
    "   - **Suggested Improvement**:\n",
    "     - Simplify the model if scaling up the dataset isnâ€™t an option (e.g., using simpler classifiers like Logistic Regression or Decision Trees).\n",
    "     - Alternatively, explore transfer learning to reduce dependency on dataset size.\n",
    "\n",
    "---\n",
    "\n",
    "### **8. Limited Real-World Email Features**\n",
    "   - **Possible Issue**: Spam classification often involves metadata beyond email text, such as:\n",
    "     - Sender reputation.\n",
    "     - Email headers (e.g., \"From\", \"Reply-To\").\n",
    "     - Attached links and domains.\n",
    "   - **Suggested Improvement**:\n",
    "     - Enrich the dataset with such features if possible.\n",
    "     - Use a multi-modal approach where text data and metadata are both included as inputs.\n",
    "\n",
    "---\n",
    "\n",
    "### **9. Lack of Robustness Testing**\n",
    "   - **Possible Issue**: Without robustness testing, the model might perform poorly when applied to unseen, real-world datasets.\n",
    "   - **Suggested Improvement**:\n",
    "     - Test the model on an **external dataset** or simulate real-world scenarios, such as:\n",
    "       - Emails with heavy use of emojis, URLs, or phishing techniques.\n",
    "       - Emails in different languages or with mixed character sets.\n",
    "\n",
    "---\n",
    "\n",
    "### **10. Not Exploring Sequential Patterns**\n",
    "   - **Possible Issue**: Emails have natural sequences of words and phrases that models like Bag-of-Words or TF-IDF might ignore.\n",
    "   - **Suggested Improvement**:\n",
    "     - Experiment with sequential models like **LSTMs**, **GRUs**, or **Transformers** to capture temporal dependencies.\n",
    "     - Alternatively, explore **n-gram features** for a middle ground.\n",
    "\n",
    "---\n",
    "\n",
    "By addressing these points, you can further refine your project and make it robust, scalable, and insightful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Platform   | Link                                           |\n",
    "|------------|------------------------------------------------|\n",
    "| GitHub     | [JackTheProgrammer](https://github.com/JackTheProgrammer) |\n",
    "| LinkedIn   | [Fawad Awan](https://www.linkedin.com/in/fawad-awan-893a58171/) |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
